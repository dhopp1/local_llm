from importlib import import_modulefrom torch import cudaimport osimport pandas as pdclass local_llm:    """Primary class of the library, use and manage the LLM, your RAG documents, and their metadata    parameters:        :embedding_model_id: str: ID of the Hugginf Face embedding model        :llm_url: str: URL for initial download of the LLM        :llm_path: str: path of the LLM locally, if available        :redownload_llm: bool: whether or not to force redownloading of the LLM model        :text_path: str: path of the directory containing the .txt files for RAG, or a list of paths of individual .txt files        :metadata_path: str: path of the metadata CSV file. The CSV must at least have the two columns, "text_id", and "file_path", containing a unique identifier for the .txt file and the location of the file        :n_gpu_layers: str: number of GPU layers to use, set '0' for cpu        :hf_token: str: Hugging Face authorization token        :temperature: float: number between 0 and 1, 0 = more conservative/less creative, 1 = more random/creative        :max_new_tokens: int: limit of how many tokens to produce for an answer        :context_window: int: 'working memory' of the LLM, varies by model    """        def __init__(        self,        embedding_model_id = "sentence-transformers/all-MiniLM-L6-v2",        llm_url = "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf",        llm_path = None,        redownload_llm = True,        text_path = None,        metadata_path = None,        hf_token = None,        n_gpu_layers = 0,        temperature = 0.0,        max_new_tokens = 512,        context_window = 3900    ):        self.model_setup = import_module("local_rag_llm.model_setup")        self.db_setup = import_module("local_rag_llm.db_setup")                self.embedding_model_id = embedding_model_id        self.llm_url = llm_url        self.llm_path = llm_path        self.text_path = text_path        self.metadata_path = metadata_path        if hf_token != None:            os.environ["HF_TOKEN"] = hf_token        self.temperature = temperature        self.max_new_tokens = max_new_tokens        self.context_window = context_window                self.device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'        self.n_gpu_layers = n_gpu_layers        self.llm = self.model_setup.instantiate_model(            text_path = self.text_path,            llm_url = self.llm_url,            llm_path = self.llm_path,            redownload_llm = redownload_llm,            temperature = self.temperature,            max_new_tokens = max_new_tokens,            context_window = context_window,            n_gpu_layers = self.n_gpu_layers        )                self.embed_model = self.db_setup.setup_embeddings(self.embedding_model_id)                # handling set up of text files and metadata csv        if self.text_path is not None:            if self.metadata_path is not None:                self.metadata = pd.read_csv(metadata_path)